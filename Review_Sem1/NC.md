# Neural Computation

## L1 Introduction

- performance P of algorithm at task T improves with experience E.
- supervised, unsupervised or reinforcement learning

## L2 Linear Regression

- linear regression models
- MSE &rarr; loss function
- optimisation
- derivatives using chain rule
- ordinary linear square
- gradient descent
  - local / global optimum

## L3 Maximum Likelihood

- probabilistic models
  - probabilistic density function
  - normal distribution
  - empirical distribution

- maximum likelihood
  - maximum likelihood estimate
  - learning via log-likelihood

## L4 Gradient Descent

- partial derivatives
- gradients
- gradient descent

## L5 Backpropagation

- computation graphs
- feedforward neural networks
- backpropagation algorithm
- local derivatives &rarr; gradient
- backwards from output layer

## L6 Softmax

- output nodes &rarr; probabilities &rarr; likelihood of a class

## L7 The Biological Brain

- brain structure &rarr; models
- learning and structural change

## L8 Optimisation

- learning rate

- SGD alternatives
  - SGD with Nesterov momentum
  - AdaGrad
  - Adam

## L9 Universal Approximation Theorem

- universal approximation
  - perceptrons and the XOR function
  - universal approximation theorem
  - power of depth

- generalisation in deep learning

## L10 Regularisation

- model capacity
- underfitting, overfitting

- regularisation techniques
  - data augmentation
  - early stopping
  - parameter norm penalties
  - dropout

## L11&12 Convolutional Networks

- convolution operation

- CNN
  - applications
    - popular for image, video and NLP etc.

  - properties
    - sparse interactions, parameter sharing

- convolutional layer
  - convolution stage
  - detector stage/ non-linearity
  - pooling stage

- backpropagation

## Compulsory Material

- lecture notes
- reading lists(excluding optional)
- videos
- exercises, lab sheets