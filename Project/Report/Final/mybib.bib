@misc{rl,
    title={Introduction to Reinforcement Learning},
    author={Richard R. Sutton and Andrew G. Barto},
    year={2018}
}

@misc{ds,
    title={UCL Reinforcement learning lectures},
    author={David Silver},
    year={2015}
}

@misc{ac,
    title={Actor-Critic Algorithms},
    author= {Vijay R. Konda and John N. Tsitsiklis},
    year={2000}}

@misc{ap,
    title={Approximation by Superpositions of a Sigmoidal Function},
    author={Cybenko},
    year={1989}}


@misc{puck,
    titile={Learning Control for Air Hockey Striking using Deep Reinforcement Learning
    },
    author={Ayal Taitler and Nahum Shimkin
    },
    year={2017}
}

@misc{td3,
    title={Addressing Function Approximation Error in Actor-Critic Methods},
    author={Scott Fujimoto and Herke van Hoof and David Meger},
    year={2018}
}

@misc{ddpgprob,
    title={The problem with DDPG: understanding failures in deterministic environments with sparse rewards},
    author={Guillaume Matheron and Nicolas Perrin and Olivier Sigaud},
    year={2019}
}

@article{pg,
    title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
    author = {Sutton, Richard and Mcallester, David},
    year = {2000},
    journal = {Adv. Neural Inf. Process. Syst}
}

@misc{gym,
    title = {OpenAI Gym},
    author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
    year = {2016}
}

@incollection{pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {PyTorch dev},
    booktitle = {Advances in Neural Information Processing Systems 32},
    year = {2019}
}

@misc{spinup,
    title = {{Spinning Up in Deep Reinforcement Learning}},
    author = {Achiam, Joshua},
    year = {2018}
}

@misc{curlingmap,
    title = {Curlingsheet flip},
    author = {Antonsusi},
    year = 2010,
    howpublished = {https://commons.wikimedia.org/w/index.php?curid=9968677}
    }

@misc{td,
    title = {An Analysis of Temporal-Difference Learning
    with Function Approximation},
    author = {John N. Tsitsiklis, Member, IEEE, and Benjamin Van Roy},
    year = {1997}
}

@misc{pg,
    title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
    author = {Richard S. Sutton and David McAllester and Satinder Singh and Yishay Mansour},
    year = {2000}
}

@misc{algorl,
    title = {Algorithms for Reinforcement Learning},
    author = {Csaba SzepesvaÃÅri},
    year = {2009}
}

@misc{atari,
    title = {Playing Atari with Deep Reinforcement Learning},
    author = {Volodymyr Mnih
    },
    year = {2015}
}

@misc{doubledqn,
    title = {Deep Reinforcement Learning with Double Q-learning},
    author = {Hado van Hasselt and Arthur Guez and David Silver},
    year = {2015}
}

@misc{drqn,
    title = {Deep Recurrent Q-Learning for Partially Observable MDPs},
    author = {Matthew Hausknecht and Peter Stone},
    year = {2017}
}

@misc{dueldqn,
    title = {Dueling Network Architectures for Deep Reinforcement Learning},
    author = {Ziyu Wang
    },
    year = {2016}
}

@misc{per,
    title = {P RIORITIZED E XPERIENCE R EPLAY},
    author = {Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
    year = {2016}
}

@misc{rainbow,
    title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
    author = {DeepMind},
    year = {2017}
}

@misc{cts,
    title = {Unifying Count-Based Exploration and Intrinsic Motivation},
    author = {DeepMind},
    year = {2016}
}

@misc{icm,
    title = {Curiosity-driven Exploration by Self-supervised Prediction},
    author = {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
    year = {2017}
}

@misc{cdl,
    title = {Large-Scale Study of Curiosity-Driven Learning},
    author = {OpenAI},
    year = {2018}
}

@misc{vime,
    title = {VIME: Variational Information Maximizing Exploration},
    author = {OpenAI},
    year = {2017}
}

@misc{meta,
    title = {L EARNING TO REINFORCEMENT LEARN},
    author = {DeepMind},
    year = {2017}
}

@misc{alphago,
    title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm
    },
    author = {DeepMind},
    year = {2017}
}

@misc{dopa,
    title = {DOPAMINE : A RESEARCH FRAMEWORK FOR DEEP REINFORCEMENT LEARNING},
    author = {Anonymous authors},
    year = {2019}
}

@misc{es,
    title = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
    author = {OpenAI},
    year = {2017}
}

@misc{a3c,
    title = {Asynchronous Methods for Deep Reinforcement Learning},
    author = {Volodymyr Mnih
    },
    year = {2016}
}

@misc{acer,
    title = {SAMPLE EFFICIENT ACTOR -CRITIC WITH EXPERIENCE REPLAY},
    author = {DeepMind},
    year = {2017}
}

@misc{ddpg,
    title = {CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING},
    author = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
    year = {2016}
}

@misc{dpg,
    title = {Deterministic Policy Gradient Algorithms
    },
    author = {David Silver and Guy Lever and Nicolas Heess and Thomas Degris and Daan Wierstra and Martin Riedmiller},
    year = {2014}
}

@misc{ppo,
    title = {Proximal Policy Optimization Algorithms},
    author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
    year = {2017}
}

@misc{sac,
    title = {Soft Actor-Critic:
    Off-Policy Maximum Entropy Deep Reinforcement
    Learning with a Stochastic Actor
    },
    author = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine },
    year = {2018}
}

@misc{trpo,
    title = {Trust Region Policy Optimization},
    author = {John Schulman and Sergey Levine and Philipp Moritz and Michael Jordan and Pieter Abbeel
    },
    year = {2017}
}

@misc{her,
    title = {Hindsight Experience Replay
    },
    author = {OpenAI},
    year = {2018}
}